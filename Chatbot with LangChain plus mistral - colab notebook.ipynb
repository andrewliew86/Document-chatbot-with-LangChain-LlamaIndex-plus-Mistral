{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d64dd6042ff2461cb708db60cfe9664b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_73b0dc302ddd449289b5220100376677",
              "IPY_MODEL_8080491b88424bb0ab61ab12f630fe74",
              "IPY_MODEL_3bb202d050ec42cb95a46326088ef652"
            ],
            "layout": "IPY_MODEL_c54869b5aa674089910aa4c7d6fc5439"
          }
        },
        "73b0dc302ddd449289b5220100376677": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7c0ceb63024413e8f39560727d82480",
            "placeholder": "​",
            "style": "IPY_MODEL_0152deb06b214fe8b58b4a4aed13f02a",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "8080491b88424bb0ab61ab12f630fe74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_038a01722d794a009dbf9d3ce46bf9f0",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_28c3f72e6e08491c9b0517d5a80e6290",
            "value": 2
          }
        },
        "3bb202d050ec42cb95a46326088ef652": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_708edcbcccf54daaa208ed513670bdfe",
            "placeholder": "​",
            "style": "IPY_MODEL_39c6dd76b4474ee5842ba49b8924753b",
            "value": " 2/2 [01:23&lt;00:00, 38.96s/it]"
          }
        },
        "c54869b5aa674089910aa4c7d6fc5439": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7c0ceb63024413e8f39560727d82480": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0152deb06b214fe8b58b4a4aed13f02a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "038a01722d794a009dbf9d3ce46bf9f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28c3f72e6e08491c9b0517d5a80e6290": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "708edcbcccf54daaa208ed513670bdfe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39c6dd76b4474ee5842ba49b8924753b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this Colab notebook, we will be performing RAG with LangChain and open source models. Instead of PDFs, we will be dealing with data directly from a website. Code was based on this medium article: https://medium.com/@thakermadhav/build-your-own-rag-with-mistral-7b-and-langchain-97d0c92fa146"
      ],
      "metadata": {
        "id": "X42I4gK219xd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOQhyK46zDoQ",
        "outputId": "fd73bc9d-b604-4398-a8da-eb38a3171f70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m794.4/794.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.4/192.4 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q torch\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git  # Installing from the source because of a known error with the Mistral model\n",
        "!pip install -q accelerate \\\n",
        "                peft \\\n",
        "                bitsandbytes \\\n",
        "                langchain \\\n",
        "                sentence_transformers \\\n",
        "                faiss-gpu \\\n",
        "                trl \\\n",
        "                playwright \\\n",
        "                html2text\n",
        "\n",
        "# Need to install playright for chromium browsing\n",
        "!playwright install"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import appropriate libraries"
      ],
      "metadata": {
        "id": "SQExPgAE3DZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
      ],
      "metadata": {
        "id": "wpMwrzst289h"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select appropriate model here"
      ],
      "metadata": {
        "id": "pRJ3RyKt2Hta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "model_config = transformers.AutoConfig.from_pretrained(model_name)\n",
        "\n",
        "# Get the approrpiate tokenizer for the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "Qyp6utHMzMEM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select appropriate quantization parameters"
      ],
      "metadata": {
        "id": "bnJs_8K52h9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = False"
      ],
      "metadata": {
        "id": "N-VMNPpS2bp7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup model"
      ],
      "metadata": {
        "id": "9K8TaBHV422P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantization config\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "# Check GPU compatibility with bfloat16\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "# Load pre-trained config\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "d64dd6042ff2461cb708db60cfe9664b",
            "73b0dc302ddd449289b5220100376677",
            "8080491b88424bb0ab61ab12f630fe74",
            "3bb202d050ec42cb95a46326088ef652",
            "c54869b5aa674089910aa4c7d6fc5439",
            "c7c0ceb63024413e8f39560727d82480",
            "0152deb06b214fe8b58b4a4aed13f02a",
            "038a01722d794a009dbf9d3ce46bf9f0",
            "28c3f72e6e08491c9b0517d5a80e6290",
            "708edcbcccf54daaa208ed513670bdfe",
            "39c6dd76b4474ee5842ba49b8924753b"
          ]
        },
        "id": "VtqwEpZr2kb5",
        "outputId": "ae4bd23a-b8eb-43e7-f373-062578c212a8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d64dd6042ff2461cb708db60cfe9664b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test out the LLM (without RAG). The answer is coherent but there are loads of inaccurate information"
      ],
      "metadata": {
        "id": "5-2bhvXr7a_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_not_chat = tokenizer.encode_plus(\"[INST] Tell me about FtsZ [/INST]\", return_tensors=\"pt\")['input_ids'].to('cuda')\n",
        "\n",
        "generated_ids = model.generate(inputs_not_chat,\n",
        "                               max_new_tokens=1000,\n",
        "                               do_sample=True)\n",
        "\n",
        "decoded = tokenizer.batch_decode(generated_ids)\n",
        "\n",
        "print(decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hUFVFeZ26-y",
        "outputId": "05974298-8627-46fe-80ed-7c783c6be5a7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s> [INST] Tell me about FtsZ [/INST] FtsZ (Flagellar filament zone protein) is a protein found in archaea and some bacteria that play a critical role in the assembly and organization of the bacterial flagell, a whip-like appendage that helps to propel the cell forward. FtsZ is a member of the peptidoglycan polymerase (PGP) enzyme family, which is responsible for the synthesis of the peptidoglycan layer that surrounds the bacterial cell and the bacterial flagellar filament.\\n\\nFtsZ is a helical protein that aligns with the axis of rotation of the flagellar motor, a complex protein machinery that powers the rotation of the flagell. FtsZ is thought to provide the basis for the polymerization of peptidoglycan filaments that make up the flagellar filament zone. Together with other proteins, FtsZ forms a ring that encircles the flagellar motor and helps to anchor the motor to the cell membrane. The assembly and disassembly of FtsZ rings are thought to be regulated by a series of highly conserved protein-protein interactions, which help to coordinate the polymerization of peptidoglycan filaments that make up the flagellar filament zone.\\n\\nRecent studies have suggested that FtsZ may have important roles in other cellular processes, including chromosome segregation, spindle assembly, and cell division. FtsZ is also thought to play a role in the assembly and stability of DNA Topoisomerases, and may be involved in the regulation of genome stability.</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up vector database\n",
        "\n",
        "Here, we will use FAISS as the vector database to store embeddings of the website (generated from sentence transformers)"
      ],
      "metadata": {
        "id": "-9LLiNgQ5Szf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.document_loaders import AsyncChromiumLoader\n",
        "from langchain.document_transformers import Html2TextTransformer\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Note: Add multiple websites if you want to build a database\n",
        "articles = [\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3439403/\"]\n",
        "\n",
        "# Scrapes the blogs above\n",
        "loader = AsyncChromiumLoader(articles)\n",
        "docs = loader.load()\n",
        "\n",
        "# Converts HTML to plain text\n",
        "html2text = Html2TextTransformer()\n",
        "docs_transformed = html2text.transform_documents(docs)\n",
        "\n",
        "# Chunk text\n",
        "text_splitter = CharacterTextSplitter(chunk_size=250,\n",
        "                                      chunk_overlap=0)\n",
        "chunked_documents = text_splitter.split_documents(docs_transformed)\n",
        "\n",
        "# Load chunked documents into the FAISS index\n",
        "# WE are using the all-mpnet-base-v2 model to generate embeddings here\n",
        "db = FAISS.from_documents(chunked_documents, HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2'))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRCqoNDk5TQE",
        "outputId": "e25660b4-5c49-4b5f-f5f9-47555f1e0185"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 308, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 366, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 373, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 261, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 313, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 279, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 275, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 259, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1700, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1205, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 816, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 880, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1166, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1130, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1128, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1319, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 660, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 968, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 761, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 606, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1136, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1203, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1940, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1103, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1047, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 749, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2038, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1366, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1353, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1323, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 624, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1321, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 688, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 556, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1089, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 759, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2406, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 928, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 679, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1165, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 898, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 787, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 719, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1341, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 773, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1249, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1261, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 831, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1139, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 723, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 561, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 587, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 809, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 442, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1532, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 308, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1140, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 384, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 311, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 580, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2607, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 369, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 980, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 350, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 357, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 297, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 263, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 259, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 387, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 295, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 270, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 278, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 259, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 267, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 255, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 251, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 281, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 259, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 274, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 306, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 281, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 289, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 252, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 255, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 270, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 278, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 266, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 317, which is longer than the specified 250\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 262, which is longer than the specified 250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup the LLM Chain and add custom instructions"
      ],
      "metadata": {
        "id": "bCVJ_QFq91tC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "text_generation_pipeline = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    temperature=0.2,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=True,\n",
        "    max_new_tokens=300,\n",
        ")\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "### [INST]\n",
        "Instruction: You are a scientist. Answer the question based on your knowledge of biomedical science. Here is context to help:\n",
        "\n",
        "{context}\n",
        "\n",
        "### QUESTION:\n",
        "{question}\n",
        "\n",
        "[/INST]\n",
        " \"\"\"\n",
        "\n",
        "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
        "\n",
        "# Create prompt from prompt template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=prompt_template,\n",
        ")\n",
        "\n",
        "# Create llm chain\n",
        "llm_chain = LLMChain(llm=mistral_llm, prompt=prompt)"
      ],
      "metadata": {
        "id": "HxeI0KsK92CS"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run LLM query againts the database\n",
        "\n",
        "Note that the top K 'context' used to generate the answers by the LLM are shown above the summarized answer"
      ],
      "metadata": {
        "id": "TiZ3pL0Z-pl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "query = \"What is the structure of FtsZ in Staphylococcuss aureus?\"\n",
        "\n",
        "# Connect query to FAISS index using a retriever\n",
        "retriever = db.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={'k': 3}\n",
        ")\n",
        "\n",
        "rag_chain = (\n",
        " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | llm_chain\n",
        ")\n",
        "\n",
        "rag_chain.invoke(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6GZjPjr-o16",
        "outputId": "67bc2d2a-6c53-4983-cb02-432e8a95a878"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 12.5 s, sys: 153 ms, total: 12.6 s\n",
            "Wall time: 12.7 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'context': [Document(page_content='11\\\\.  Löwe J, Amos LA (1998) Crystal structure of the bacterial cell-division\\nprotein FtsZ. Nature 391: 203–206. [PubMed] [Google Scholar]', metadata={'source': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3439403/'}),\n",
              "  Document(page_content='11\\\\.  Löwe J, Amos LA (1998) Crystal structure of the bacterial cell-division\\nprotein FtsZ. Nature 391: 203–206. [PubMed] [Google Scholar] [Ref list]', metadata={'source': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3439403/'}),\n",
              "  Document(page_content='### 3D-SIM of Z Rings in _S. aureus_ Cells Also Reveals a Heterogeneous\\nDistribution of FtsZ', metadata={'source': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3439403/'})],\n",
              " 'question': 'What is the structure of FtsZ in Staphylococcuss aureus?',\n",
              " 'text': '\\nAccording to the provided documents, the crystal structure of FtsZ in Staphylococcus aureus has been determined through X-ray crystallography and 3D-SIM imaging. The crystal structure reveals that FtsZ forms a hexameric ring with six subunits arranged in a helical fashion. Each subunit contains a conserved GTP-binding motif, which is responsible for the dynamic polymerization and depolymerization of FtsZ during cell division. Additionally, the 3D-SIM images show that FtsZ forms Z-rings in S. aureus cells, which are important for the proper segregation of chromosomes during cell division. However, the distribution of FtsZ within these Z-rings appears to be heterogeneous, suggesting that other proteins may also play a role in the process of chromosome segregation.'}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optional:\n",
        "\n",
        "Here is the code if you want to query a PDF with LangChain"
      ],
      "metadata": {
        "id": "_3SG_QVgDIcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"my_pdf_file.pdf\")\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "texts = loader.load_and_split(text_splitter)\n",
        "\n",
        "# pass the text and embeddings to FAISS\n",
        "db = FAISS.from_documents(texts, HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2'))\n"
      ],
      "metadata": {
        "id": "xT7__7jmDHv1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}