{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "We will be using open source libraries and large language models (llama index and Mistral-7b) to perform Q&A with PDF articles. LlamaIndex is a data framework for connecting custom data sources to LLMs.\n",
        "\n",
        "We will be running the code using Google Colab. Please make sure that the T4 GPU instance of Colab notebook is activated via the notebook settings before proceeding.\n",
        "\n",
        "Code is based on this tutorial: https://www.youtube.com/watch?v=1mH1BvBJCl0. See also llama index documentation here: https://docs.llamaindex.ai/en/stable/examples/llm/llama_2_llama_cpp.html"
      ],
      "metadata": {
        "id": "DmEscT8vtf98"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "as3sDIQltNls",
        "outputId": "9b8ac861-6a41-40c3-9bbf-6879d681efd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.10/dist-packages (0.2.24)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.9.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.23.5)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q pypdf\n",
        "!pip install -q python-dotenv\n",
        "!pip install -q transformers\n",
        "!pip install -q llama-index\n",
        "!pip install -q sentence-transformers\n",
        "!pip install -q langchain\n",
        "# Ask Llama to use GPU\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logging\n",
        "import logging\n",
        "import sys\n",
        "# Format the response from LLM so they display properly\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "# import LLama index related functions\n",
        "import torch\n",
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
        "from llama_index.llms import LlamaCPP\n",
        "from llama_index.llms.llama_utils import messages_to_prompt, completion_to_prompt  # pass in messages_to_prompt and completion_to_prompt functions to help format the model inputs.\n",
        "\n",
        "# Import embedding libraries and functions\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from llama_index.embeddings import LangchainEmbedding"
      ],
      "metadata": {
        "id": "xuzgVpo8vdE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Turn on logging"
      ],
      "metadata": {
        "id": "R728k4ERx7BH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure logging is turned\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
      ],
      "metadata": {
        "id": "eDrg6iM8vfeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load in your Mistral model here. We will be using the fine-tuned, quantized Mistral 7b model for everything"
      ],
      "metadata": {
        "id": "DM8lpNF7xcGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = LlamaCPP(\n",
        "    # You can pass in the URL to a GGUF model to download it automatically\n",
        "    model_url='https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf',\n",
        "    model_path=None,\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=256,\n",
        "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
        "    context_window=3900,\n",
        "    # kwargs to pass to __call__()\n",
        "    generate_kwargs={},\n",
        "    # kwargs to pass to __init__()\n",
        "    # set to at least 1 to use GPU\n",
        "    model_kwargs={\"n_gpu_layers\": -1},\n",
        "    # transform inputs into Llama2 format\n",
        "    messages_to_prompt=messages_to_prompt,\n",
        "    completion_to_prompt=completion_to_prompt,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-l-1K6x4xY6o",
        "outputId": "266f65db-fc4e-4dfe-87e7-3723a1f5a8f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading url https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf to path /tmp/llama_index/models/mistral-7b-instruct-v0.1.Q4_K_M.gguf\n",
            "total size (MB): 4368.44\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "4167it [00:22, 184.10it/s]                          \n",
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load your embedding model and specify the directory containing your PDFs"
      ],
      "metadata": {
        "id": "8VFAuX0Hy_A3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change embedding model to suit your needs. A list of embedding models from Hugging Face can be found here: https://huggingface.co/models?pipeline_tag=sentence-similarity&sort=trending\n",
        "embed_model = LangchainEmbedding(\n",
        "  HuggingFaceEmbeddings(model_name=\"thenlper/gte-large\")\n",
        ")\n",
        "\n",
        "# Place your PDF articles that will be used for Q&A in a folder called 'Data'. The PDF reader will extract the PDF into text\n",
        "documents = SimpleDirectoryReader(\"/content/Data/\").load_data()"
      ],
      "metadata": {
        "id": "E9WO1WUyy_Hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specify service context. The default model for llama_index is from OpenAI, here we specify that we are using our custom LLM and embedding model. Chunk size tells us the size of the context that will be sent to the model."
      ],
      "metadata": {
        "id": "pSbylUumzLGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "service_context = ServiceContext.from_defaults(\n",
        "    chunk_size=256,\n",
        "    llm=llm,\n",
        "    embed_model=embed_model\n",
        ")\n",
        "\n",
        "# Initiate the vector store\n",
        "index = VectorStoreIndex.from_documents(documents, service_context=service_context)"
      ],
      "metadata": {
        "id": "dVbP52GvzLOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform your query\n",
        "\n",
        "The PDF I uploaded was my scientific paper on FtsZ dynamics. It is quite technical and could be challenging for the model.\n",
        "\n",
        "In cases where you want to use the top k chunks refer to documentation here: https://docs.llamaindex.ai/en/stable/understanding/querying/querying.html#customizing-the-stages-of-querying"
      ],
      "metadata": {
        "id": "8-2pAskX0-RU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What are the similarities between Z ring structures in S. aureus and B. subtilis?\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "6fHRaSZX09iY",
        "outputId": "3737201a-9d53-438d-cb5d-ab35ca1a2397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b> Based on the provided context information, it appears that there are some similarities between Z ring structures in S. aureus and B. subtilis. When imaged in the axial plane, S. aureus Z rings were almost identical to typical 3D-SIM images of B. subtilis Z rings acquired in the same orientation (Figure 3E, Movie S2). This suggests that both organisms have similar overall structures for their Z rings. However, when imaged in the lateral plane, there are differences between the two organisms. In S. aureus, there was a heterogeneous, bead-like distribution of FtsZ throughout the entire Z ring, with visible \"gaps\" often observed within the ring (Figure 3D and 3F). On the other hand, in B. subtilis, the lateral plane imaging showed a more uniform distribution of FtsZ within the Z ring. Additionally, the concentration of FtsZ-GFP within the ring can typically vary by up to 4-fold in S. aureus (Figure 3G), while in B. subtilis, there is no mention of such variation. Therefore, while there are</b>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 28.5 s, sys: 32 ms, total: 28.6 s\n",
            "Wall time: 28.7 s\n"
          ]
        }
      ]
    }
  ]
}